# ==============================================
# LangChain + Ollama enrichment configuration
# These options are read by scripts in projects/project-5
# ==============================================

# Prompt configuration Markdown
DEFINITIONS_PROMPT_CONFIG_FILE=prompts/preprocessing_definitions_prompts.md

# OLLAMA model configuration
OLLAMA_MODEL=gemma3n

# Sampling temperature (0.0–1.0). Lower = more deterministic outputs.
OLLAMA_TEMPERATURE=0.2

# Reference glossary injection (BFO/CCO) — lightweight RAG context
# How to include BFO/CCO reference terms in the prompt:
#   off      -> do not include any reference context
#   retrieve -> per row, select top‑K entries via simple token overlap
#   fuzzy    -> per row, select top‑K entries using RapidFuzz (recommended)
#   vector   -> per row, select top‑K entries using vector embedding
REFERENCE_MODE=vector

# Number of reference entries to inject per row (for retrieve/fuzzy modes)
REF_TOP_K=15

# Max characters of injected reference context to keep prompts compact
REF_MAX_CHARS=4000

# Fuzzy retrieval (RapidFuzz) — used only when REFERENCE_MODE=fuzzy
# Scorer options:
#   token_set_ratio  -> robust to extra/missing words (default)
#   token_sort_ratio -> robust to word order changes
#   wratio           -> overall smart scorer
REF_FUZZY_SCORER=token_set_ratio

# Cutoff score (0–100). Higher = stricter matching, fewer references.
REF_FUZZY_CUTOFF=70

# Post-processing: enforce capitalization of overlapping glossary labels
# If true, after generation we will replace case-insensitive matches of retrieved
# BFO/CCO labels in the output with their canonical casing. This guarantees
# capitalization on overlapping word combinations.
# Options: true|false (1/0/yes/no/on/off supported)
ENFORCE_LABEL_CASING=true

# Debugging: echo fully-rendered prompts per row (including fuzzy context)
# If enabled, the script will log the prompt (system + user messages) and
# optionally append them to a file. Useful for auditing what the LLM sees.
# Options: true|false (1/0/yes/no/on/off supported)
ECHO_PROMPTS=true
# Where to write prompts (relative to project-5 if not absolute). Example:
ECHO_PROMPTS_FILE=logs/prompt_echo.txt

# Note: File paths are fixed in the script and do not need .env entries:
# - INPUT_FILE:   projects/project-5/data/definitions.csv
# - OUTPUT_FILE:  projects/project-5/data/definitions_enriched.csv
# - BFO_CCO_TERMS: projects/project-5/data/bfo_cco_terms.csv

# ==============================================
# Ollama GPU selection (optional)
# ==============================================
# Ollama itself does not take a GPU index flag; instead, you pin the server
# process to a particular device using CUDA/HIP visibility environment vars.
# Use scripts/start_ollama_gpu.sh to apply these conveniently.
#
# For NVIDIA:
#   Set CUDA_VISIBLE_DEVICES to select which GPUs are visible to Ollama.
#   Example to use only GPU 1 (second GPU):
#   CUDA_VISIBLE_DEVICES=1
#
# For AMD (ROCm):
#   Use HIP_VISIBLE_DEVICES similarly.
#
# These are read by scripts/start_ollama_gpu.sh and exported before
# running `ollama serve`. Leave empty to use all GPUs.
OLLAMA_CUDA_VISIBLE_DEVICES=0
OLLAMA_HIP_VISIBLE_DEVICES=

# ==============================================
# ROBOT + ELK reasoning configuration
# ==============================================

# Path to the ROBOT executable; if installed system-wide, keep as 'robot'.
# You may also point to a local wrapper script (e.g., projects/project-4/assignment/robot/robot)
ROBOT_BIN=robot

# Reasoner to use with ROBOT's reason command (default: ELK)
ELK_REASONER=ELK

# Default input ontology for reasoning (can be overridden at CLI)
# Relative to the project-5 directory when used by scripts/reason_with_elk.sh
ELK_INPUT=src/InformationEntityOntology.ttl

# Default output path for reasoned ontology
ELK_OUTPUT=src/module_reasoned.ttl

# Optional Java memory settings for ROBOT, uncomment to set
# ROBOT_JAVA_ARGS=-Xms2G -Xmx4G

# Optional: control ROBOT auto-download in reason_with_elk.sh
# If ROBOT is not found on PATH and ROBOT_BIN doesn't exist, the script will
# download robot.jar to ROBOT_DIR and create a wrapper at ROBOT_BIN (or
# ${ROBOT_DIR}/robot if ROBOT_BIN is just a name).
 ROBOT_VERSION=1.9.7
 ROBOT_DIR=robot


 # ==============================================
 # ROBOT diff configuration (used by scripts/diff_with_robot.sh)
 # ==============================================

 # Left and right files for diff (defaults target the IEO example in this project)
 # Relative to the project-5 directory
 DIFF_LEFT=src/InformationEntityOntology.ttl
 DIFF_RIGHT=src/module_reasoned.ttl

 # Output format for robot diff: md|html|owl|json (see ROBOT docs)
 DIFF_FORMAT=md

 # Output path
 DIFF_OUTPUT=reports/elk_diff.md
