# ==============================================
# LangChain + Ollama enrichment configuration
# These options are read by scripts in projects/project-5
# ==============================================

# OLLAMA model configuration
# - Name of the local model served by `ollama serve`
# - Examples: llama3.1, mistral, qwen2.5, phi3, etc.
OLLAMA_MODEL=llama3.1

# Sampling temperature (0.0–1.0). Lower = more deterministic outputs.
OLLAMA_TEMPERATURE=0.2

# Reference glossary injection (BFO/CCO) — lightweight RAG context
# How to include BFO/CCO reference terms in the prompt:
#   off      -> do not include any reference context
#   full     -> include (truncated) full glossary once in the system prompt
#   retrieve -> per row, select top‑K entries via simple token overlap
#   fuzzy    -> per row, select top‑K entries using RapidFuzz (recommended)
REFERENCE_MODE=fuzzy

# Number of reference entries to inject per row (for retrieve/fuzzy modes)
REF_TOP_K=5

# Max characters of injected reference context to keep prompts compact
REF_MAX_CHARS=4000

# Fuzzy retrieval (RapidFuzz) — used only when REFERENCE_MODE=fuzzy
# Scorer options:
#   token_set_ratio  -> robust to extra/missing words (default)
#   token_sort_ratio -> robust to word order changes
#   wratio           -> overall smart scorer
REF_FUZZY_SCORER=token_set_ratio

# Cutoff score (0–100). Higher = stricter matching, fewer references.
REF_FUZZY_CUTOFF=70

# Post-processing: enforce capitalization of overlapping glossary labels
# If true, after generation we will replace case-insensitive matches of retrieved
# BFO/CCO labels in the output with their canonical casing. This guarantees
# capitalization on overlapping word combinations.
# Options: true|false (1/0/yes/no/on/off supported)
ENFORCE_LABEL_CASING=true

# Note: File paths are fixed in the script and do not need .env entries:
# - INPUT_FILE:   projects/project-5/data/definitions.csv
# - OUTPUT_FILE:  projects/project-5/data/definitions_enriched.csv
# - BFO_CCO_TERMS: projects/project-5/data/bfo_cco_terms.csv
